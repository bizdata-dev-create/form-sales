# ---
# jupyter:
#   jupytext:
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.17.3
#   kernelspec:
#     display_name: base
#     language: python
#     name: python3
# ---

# %% [markdown] id="8RnUTOYIaFBb"
# # å•ã„åˆã‚ã›URLå–å¾—

# %% [markdown] id="X6ZutDC4VoIp"
# ## é–¢æ•°ç¾¤

# %% [markdown]
# ### ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã®èª¿æ•´

# %%
##xxxå®Ÿé¨“----------------------------------------------

# %%
import logging
import warnings

# ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚’WARNINGä»¥ä¸Šã«è¨­å®šï¼ˆDEBUGã¨INFOã‚’éè¡¨ç¤ºï¼‰
logging.getLogger().setLevel(logging.WARNING)

# ç‰¹å®šã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ãƒ­ã‚°ã‚’ç„¡åŠ¹åŒ–
logging.getLogger('httpcore').setLevel(logging.WARNING)
logging.getLogger('httpx').setLevel(logging.WARNING)
logging.getLogger('openai').setLevel(logging.WARNING)

# è­¦å‘Šã‚‚éè¡¨ç¤ºã«ã™ã‚‹å ´åˆ
warnings.filterwarnings('ignore')

print("ãƒ­ã‚°è¨­å®šå®Œäº†")

# %% colab={"base_uri": "https://localhost:8080/"} id="ptwJFOn7p4DD" outputId="47bde079-b9f8-48eb-c130-0a0339c0aa49"
# â”€â”€â”€ å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# !pip install google-api-python-client beautifulsoup4
# !pip -q install openpyxl odfpy
# !pip -q install gspread gspread_dataframe

# %% id="4y2w5yroWJuW"
import os
from pathlib import Path
from dotenv import load_dotenv, find_dotenv
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç›´ä¸‹ã® .env ã‚’ç‰¹å®šã—ã¦ä¸Šæ›¸ãèª­ã¿è¾¼ã¿
project_root = next(p for p in [Path.cwd(), *Path.cwd().parents] if (p / ".env").exists())
env_file = str(project_root / ".env")
load_dotenv(dotenv_path=env_file, override=True)

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", "")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "")
CSE_ID = os.getenv("CSE_ID", "")
# cse = make_search_client(GOOGLE_API_KEY, CSE_ID)

import os

# %%
import sys
print(sys.executable)

# %% [markdown]
# ## å•ã„åˆã‚ã›å–å¾—

# %% id="j9Q1McZspUh7"
# â”€â”€â”€ ã‚¤ãƒ³ãƒãƒ¼ãƒˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import pandas as pd
import requests
import logging
import time
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from googleapiclient.discovery import build
from typing import Optional, Tuple, List
from datetime import datetime
from tqdm.auto import tqdm
from datetime import datetime

# â”€â”€â”€ ãƒ­ã‚®ãƒ³ã‚°è¨­å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(level=logging.DEBUG,
                    format='%(asctime)s %(levelname)s %(message)s')

# â”€â”€â”€ Google Custom Search API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä½œæˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def make_search_client(api_key: str, cse_id: str):
    logging.debug("Creating Custom Search client")
    client = build("customsearch", "v1", developerKey=api_key).cse()
    logging.debug("Custom Search client created")
    return client

# â”€â”€â”€ ä¼šç¤¾åã§å…¬å¼ã‚µã‚¤ãƒˆ URL ã‚’å–å¾— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_hp_url(company_name: str, cse_client, cse_id: str) -> str:
    logging.debug(f"Searching HP URL for: {company_name}")
    try:
        res = cse_client.list(q=company_name, cx=cse_id, num=1).execute()
        items = res.get("items", [])
        hp = items[0]["link"] if items else None
        logging.debug(f"â†’ HP URL found: {hp}")
        return hp
    except Exception as e:
        logging.error(f"Error fetching HP URL for {company_name}: {e}")
        return None

# â”€â”€â”€ ãƒšãƒ¼ã‚¸ãŒãƒ•ã‚©ãƒ¼ãƒ ã‹åˆ¤å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def is_form_page(soup: BeautifulSoup) -> bool:

    text_types ={"text","email","tel","number"}
    inputs = soup.find_all(
        "input",
        attrs={
            "type": lambda t: t and t.lower() in text_types,
            "name": True
        }
    )
    return len(inputs) >= 3

    return True

# â”€â”€â”€ å•ã„åˆã›URLå–å¾— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_contact_url(hp_url: str, timeout: float = 5.0) -> Optional[str]:
    """
    hp_url ã‹ã‚‰æœ€å¤§æ·±åº¦3ã¾ã§ãƒªãƒ³ã‚¯ã‚’ãŸã©ã‚Šã€
    ãŠå•ã„åˆã‚ã›ãƒ•ã‚©ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ã¨åˆ¤æ–­ã§ããŸ URL ã‚’è¿”ã™ã€‚
    è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã° Noneã€‚
    """
    logging.debug(f"Searching contact URL on: {hp_url}")
    if not hp_url:
        logging.warning("No HP URL provided, skipping contact search")
        return None

    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å®šç¾©
    primary_kw   : List[str] = ["å•ã„åˆã‚ã›", "ãŠå•ã„åˆã‚ã›", "å•åˆã‚ã›", "å•ã„åˆã›", "ã‚³ãƒ³ã‚¿ã‚¯ãƒˆ", "contact", "inquiry", "request", "entry"]
    secondary_kw : List[str] = ["ãƒ•ã‚©ãƒ¼ãƒ ", "ãã®ä»–", "æ¡ç”¨", "IR", "æœ¬éƒ¨"] + primary_kw

    session = requests.Session()

    def fetch_soup(url: str) -> Tuple[Optional[BeautifulSoup], Optional[str]]:
        """ URL ã‚’ GET ã—ã¦ (soup, æœ€çµ‚çš„ãªçµ¶å¯¾URL) ã‚’è¿”ã™ """
        try:
            resp = session.get(url, timeout=timeout)
            resp.raise_for_status()
            return BeautifulSoup(resp.text, "html.parser"), resp.url
        except Exception as e:
            # logging.warning(f"  â†’ Failed to fetch {url}: {e}")
            return None, None

    def extract_links(soup: BeautifulSoup, base_url: str, kws: List[str]) -> List[str]:
        """
        <a href> ã® text or href ã«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹ã‚‚ã®ã‚’æŠ½å‡ºã—ã€
        çµ¶å¯¾URLã§è¿”ã™
        """
        results: List[str] = []
        for a in soup.find_all("a", href=True):
            text = a.get_text(strip=True).lower()
            href = a["href"].lower()
            if any(kw in text for kw in kws) or any(kw in href for kw in kws):
                abs_url = urljoin(base_url, a["href"])
                logging.debug(f"   â†’ Candidate link: {abs_url}")
                results.append(abs_url)
        return results

    # æ·±åº¦ã”ã¨ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒ—
    kw_by_depth = {1: primary_kw, 2: secondary_kw, 3: secondary_kw}

    # BFS ã§æœ€å¤§æ·±åº¦3ã¾ã§æ¢ç´¢
    frontier = [(hp_url, 0)]  # (URL, depth)
    visited  = set()

    while frontier:
        url, depth = frontier.pop(0)
        if url in visited or depth >= 3:
            continue
        visited.add(url)

        soup, real_url = fetch_soup(url)
        if soup is None:
            continue

        logging.debug(f"[depth={depth}] Visiting: {real_url}")

        # depth>0 ã®ãƒšãƒ¼ã‚¸ã§ãƒ•ã‚©ãƒ¼ãƒ åˆ¤å®š
        if depth > 0 and is_form_page(soup):
            # logging.info(f"Contact form URL found at depth {depth}: {real_url}")
            # print(f"Contact form URL found at depth {depth}: {real_url}")
            return real_url

        # æ¬¡ã®æ·±åº¦ã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºã—ã¦ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
        next_depth = depth + 1
        kws = kw_by_depth.get(next_depth, [])
        if not kws:
            continue

        for link in extract_links(soup, real_url, kws):
            frontier.append((link, next_depth))

    logging.info("Contact form URL not found within depth 3")
    return None

def fill_contact_from_hp(df):
    mask = df['contact_url'].isna() & df['hp_url'].str.contains(r'contact|inquiry|toiawase|ãŠå•ã„åˆã‚ã›|ãŠå•åˆã›', case=False, na=False)
    df.loc[mask, 'contact_url'] = df.loc[mask, 'hp_url']
    return df


# â”€â”€â”€ DataFrame ã«å¯¾ã—ã¦ä¸€æ‹¬å‡¦ç† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fill_urls(df: pd.DataFrame, api_key: str, cse_id: str) -> pd.DataFrame:
    cse = make_search_client(api_key, cse_id)
    hp_urls = []
    contact_urls = []

    for i, name in tqdm(enumerate(df["company_name"], start=1)):
        # print(f"=== å‡¦ç†é–‹å§‹ {i}/{len(df)}: {name} ===")
        # logging.info(f"=== å‡¦ç†é–‹å§‹ {i}/{len(df)}: {name} ===")

        # å…¬å¼ã‚µã‚¤ãƒˆ URL
        hp = get_hp_url(name, cse, cse_id)
        hp_urls.append(hp)

        # å•ã„åˆã‚ã›ãƒ•ã‚©ãƒ¼ãƒ  URL
        contact = get_contact_url(hp)
        contact_urls.append(contact)

        logging.info(f"â†’ çµæœ: HP={hp}, Contact={contact}\n")

        time.sleep(1)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–

    df["hp_url"] = hp_urls
    df["contact_url"] = contact_urls
    return df


# %%
from dotenv import load_dotenv
load_dotenv()  # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç›´ä¸‹ã® .env ã‚’èª­ã¿è¾¼ã‚€

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", "")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "")
CSE_ID = os.getenv("CSE_ID", "")
cse = make_search_client(GOOGLE_API_KEY, CSE_ID)
print("google_api_key:",GOOGLE_API_KEY)
print(GOOGLE_API_KEY)
names = ["ã‚½ãƒ‹ãƒ¼","Apple"]
cse = make_search_client(GOOGLE_API_KEY, CSE_ID)
for name in names:
    hp = get_hp_url(name, cse, CSE_ID)
    print("HP:",hp)

# %% [markdown] id="hAK_42aaWSrB"
# ## ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆä¸Šä¸æ˜ã®ã‚‚ã®ã‚’ä¿å­˜

# %% id="ckdsDSCmjO8K"
# è¿½åŠ : ãƒãƒƒã‚·ãƒ¥ä»˜ãã‚¢ãƒ³ã‚«ãƒ¼ã‚‚å€™è£œã«å…¥ã‚Œã‚‹
COMMON_RELATIVE_PATHS = [
    "/contact",
    "/contact-us",
    "/contacact.html",
    "/contact/other/",
    "/contact/others/",
    "/contact/form",
    "/contact/recruit/",
    "/inquiry",
    "/inquiries",
    "/request",
    "/requests",
    "#contact",            # â† ã“ã‚Œã‚’è¿½åŠ 
]

from urllib.parse import urljoin
from bs4 import BeautifulSoup
import requests
import pandas as pd
import re
from tqdm import tqdm

def fill_contact_url(df: pd.DataFrame, timeout: float = 7.0) -> pd.DataFrame:
    null_mask = df["contact_url"].isna() | df["contact_url"].astype(str).str.strip().eq("")
    print(null_mask)
    for idx, row in tqdm(df[null_mask].iterrows(), total=null_mask.sum()):
        base_url = row.get("hp_url")
        if not base_url:
            continue
        # ãƒ™ãƒ¼ã‚¹ãƒšãƒ¼ã‚¸ã¯1å›ã ã‘å–å¾—ï¼ˆ#fragmentç”¨ï¼‰
        soup_home = None
        try:
            res_home = requests.get(base_url, timeout=timeout)
            if res_home.status_code == 200:
                soup_home = BeautifulSoup(res_home.content, "html.parser")
        except Exception as e:
            print(f"Error fetching base page {base_url}: {e}")

        tested = set()
        found = False

        for path in COMMON_RELATIVE_PATHS:
            # ãƒãƒƒã‚·ãƒ¥ï¼ˆ#...ï¼‰ã¯ãƒšãƒ¼ã‚¸å†…ã‚¢ãƒ³ã‚«ãƒ¼æ‰±ã„
            if path.startswith("#"):
                if soup_home is None:
                    continue  # ãƒ›ãƒ¼ãƒ å–å¾—å¤±æ•—æ™‚ã¯ã‚¹ã‚­ãƒƒãƒ—
                # è©²å½“ã‚¢ãƒ³ã‚«ãƒ¼ãŒå­˜åœ¨ã™ã‚‹ã‹
                target = soup_home.select_one(path)  # ä¾‹: '#contact'
                if not target:
                    continue

                # ã‚»ã‚¯ã‚·ãƒ§ãƒ³å˜ä½ã§ãƒ•ã‚©ãƒ¼ãƒ ã‚‰ã—ã•ã‚’åˆ¤å®šï¼ˆã ã‚ãªã‚‰ãƒšãƒ¼ã‚¸å…¨ä½“ã§åˆ¤å®šï¼‰
                try:
                    if is_form_page(target) or is_form_page(soup_home):
                        contact_url = urljoin(base_url, path)
                        df.at[idx, "contact_url"] = contact_url
                        print(f"Found contact URL (fragment): {contact_url}")
                        found = True
                        break
                except Exception as e:
                    # is_form_page ãŒ Tag ã‚’æƒ³å®šã—ã¦ã„ãªã„å ´åˆã¯ãƒšãƒ¼ã‚¸å…¨ä½“ã§å†åˆ¤å®š
                    try:
                        if is_form_page(soup_home):
                            contact_url = urljoin(base_url, path)
                            df.at[idx, "contact_url"] = contact_url
                            print(f"Found contact URL (fragment-fallback): {contact_url}")
                            found = True
                            break
                    except Exception as ee:
                        print(f"is_form_page error on fragment {path}: {ee}")
                continue

            # é€šå¸¸ã®ç›¸å¯¾ãƒ‘ã‚¹ã¯ä»Šã¾ã§é€šã‚Šå–å¾—ã—ã¦åˆ¤å®š
            test_url = urljoin(base_url, path)
            if test_url in tested:
                continue
            tested.add(test_url)

            try:
                res = requests.get(test_url, timeout=timeout)
                if res.status_code == 200:
                    soup = BeautifulSoup(res.content, "html.parser")
                    if is_form_page(soup):
                        df.at[idx, "contact_url"] = test_url
                        print(f"Found contact URL: {test_url}")
                        found = True
                        break  # æœ€åˆã«è¦‹ã¤ã‘ãŸãƒ•ã‚©ãƒ¼ãƒ ã§çµ‚äº†
            except Exception as e:
                print(f"Error fetching {test_url}: {e}")
                continue  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚„æ¥ç¶šã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–

        # è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ã€ãƒ›ãƒ¼ãƒ å†…ã®<a href="#...">ã‹ã‚‰#contactç³»ã‚’è£œè¶³ï¼ˆä¿é™ºï¼‰
        if not found and soup_home is not None:
            try:
                anchors = soup_home.select('a[href^="#"]')
                for a in anchors:
                    href = a.get("href", "")
                    text = (a.get_text() or "") + " " + href
                    if re.search(r"(contact|inquiry|ãŠå•ã„åˆã‚ã›|ãŠå•åˆã›|å•åˆã›)", text, re.I):
                        target = soup_home.select_one(href)
                        if target and (is_form_page(target) or is_form_page(soup_home)):
                            contact_url = urljoin(base_url, href)
                            df.at[idx, "contact_url"] = contact_url
                            print(f"Found contact URL (fragment-auto): {contact_url}")
                            break
            except Exception as e:
                print(f"Error scanning fragments on {base_url}: {e}")

    return df


# %% id="n2bNifpYYm0a"
# === æ”¹å–„ç‰ˆã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆé–¢æ•°ï¼ˆAPIåˆ¶é™å¯¾ç­–ä»˜ãï¼‰ ===
def export_unknown_contacts_to_gsheet_improved(df, spreadsheet_id, sheet_name):
    """
    æ”¹å–„ç‰ˆï¼šå•ã„åˆã‚ã›URLãŒæœªå–å¾—ã®ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã‚’Google Sheetsã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹
    ï¼ˆæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒã—ã¦æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ã€APIåˆ¶é™å¯¾ç­–ä»˜ãï¼‰
    """
    try:
        import gspread
        from gspread_dataframe import set_with_dataframe
        from google.oauth2.service_account import Credentials
        import os
        import time
        
        # èªè¨¼æƒ…å ±ã®è¨­å®š
        scope = [
            'https://spreadsheets.google.com/feeds',
            'https://www.googleapis.com/auth/drive',
            'https://www.googleapis.com/auth/spreadsheets'
        ]
        
        # ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’ç¢ºèª
        service_account_key_path = r"C:\Users\qingy\Documents\è‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ å–¶æ¥­äº‹æ¥­\form-sales-log-bffd68dc6996.json"
        
        if not os.path.exists(service_account_key_path):
            print(f"âŒ ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {service_account_key_path}")
            return export_unknown_contacts_to_csv(df)
        
        # ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
        credentials = Credentials.from_service_account_file(
            service_account_key_path, 
            scopes=scope
        )
        gc = gspread.authorize(credentials)
        
        print(f"âœ… èªè¨¼æˆåŠŸ: {credentials.service_account_email}")
        
        # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚’é–‹ã
        try:
            spreadsheet = gc.open_by_key(spreadsheet_id)
            print(f"âœ… ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚¢ã‚¯ã‚»ã‚¹æˆåŠŸ: {spreadsheet.title}")
        except Exception as e:
            print(f"âŒ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
            return export_unknown_contacts_to_csv(df)
        
        # ã‚·ãƒ¼ãƒˆã‚’å–å¾—ã¾ãŸã¯ä½œæˆ
        try:
            worksheet = spreadsheet.worksheet(sheet_name)
            print(f"âœ… æ—¢å­˜ã‚·ãƒ¼ãƒˆã‚’ä½¿ç”¨: {sheet_name}")
        except gspread.WorksheetNotFound:
            worksheet = spreadsheet.add_worksheet(title=sheet_name, rows=1000, cols=20)
            print(f"âœ… æ–°è¦ã‚·ãƒ¼ãƒˆã‚’ä½œæˆ: {sheet_name}")
        
        # å•ã„åˆã‚ã›URLãŒæœªå–å¾—ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        unknown_contacts = df[
            df["contact_url"].isna() | 
            (df["contact_url"].str.strip() == "") |
            (df["contact_url"] == "None")
        ].copy()
        
        if len(unknown_contacts) == 0:
            print("âœ… ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹æœªå–å¾—ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“")
            return
        
        print(f"ğŸ“Š ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå¯¾è±¡: {len(unknown_contacts)}ä»¶")
        
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
        existing_data = worksheet.get_all_values()
        print(f"ğŸ“‹ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿è¡Œæ•°: {len(existing_data)}")
        
        # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ï¼ˆæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒï¼‰
        if len(existing_data) == 0:
            # ã‚·ãƒ¼ãƒˆãŒç©ºã®å ´åˆã€ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
            print("ï¿½ï¿½ ç©ºã®ã‚·ãƒ¼ãƒˆã«ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ã—ã¾ã™")
            set_with_dataframe(worksheet, unknown_contacts)
        else:
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã€ãƒãƒƒãƒå‡¦ç†ã§ä¸€æ‹¬è¿½åŠ ï¼ˆAPIåˆ¶é™å¯¾ç­–ï¼‰
            print("ï¿½ï¿½ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®æœ€çµ‚è¡Œã‹ã‚‰æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ‹¬è¿½åŠ ã—ã¾ã™")
            
            # æœ€çµ‚è¡Œã®è¡Œç•ªå·ã‚’å–å¾—
            next_row = len(existing_data) + 1
            
            # ãƒãƒƒãƒå‡¦ç†ã§ä¸€æ‹¬è¿½åŠ ï¼ˆAPIåˆ¶é™å¯¾ç­–ï¼‰
            batch_data = []
            for row in unknown_contacts.values:
                # ãƒ‡ãƒ¼ã‚¿ã‚’æ–‡å­—åˆ—ã¨ã—ã¦å¤‰æ›ï¼ˆNoneã‚’ç©ºæ–‡å­—åˆ—ã«ï¼‰
                row_data = [str(val) if val is not None else "" for val in row]
                batch_data.append(row_data)
            
            # ä¸€æ‹¬ã§ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
            worksheet.update(f'A{next_row}', batch_data)
            
            # APIåˆ¶é™å¯¾ç­–ã®ãŸã‚å°‘ã—å¾…æ©Ÿ
            time.sleep(2)
            
            print(f"âœ… {len(unknown_contacts)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’{next_row}è¡Œç›®ã‹ã‚‰ä¸€æ‹¬è¿½åŠ ã—ã¾ã—ãŸ")
        
        print(f"ğŸ‰ å®Œäº†: {len(unknown_contacts)}ä»¶ã®æœªå–å¾—ãƒ‡ãƒ¼ã‚¿ã‚’{sheet_name}ã‚·ãƒ¼ãƒˆã«è¿½åŠ ã—ã¾ã—ãŸ")
        
    except Exception as e:
        print(f"âŒ Google Sheetsã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        print("CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¾ã™...")
        export_unknown_contacts_to_csv(df)

def export_unknown_contacts_to_csv(df, filename=None):
    """
    CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹é–¢æ•°ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰
    """
    if filename is None:
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"unknown_contacts_{timestamp}.csv"
    
    # å•ã„åˆã‚ã›URLãŒæœªå–å¾—ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    unknown_contacts = df[
        df["contact_url"].isna() | 
        (df["contact_url"].str.strip() == "") |
        (df["contact_url"] == "None")
    ].copy()
    
    if len(unknown_contacts) == 0:
        print("âœ… ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹æœªå–å¾—ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“")
        return
    
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    unknown_contacts.to_csv(filename, index=False, encoding='utf-8-sig')
    print(f"ğŸ“ CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã—ãŸ: {filename}")
    print(f"ï¿½ï¿½ ä¿å­˜ä»¶æ•°: {len(unknown_contacts)}ä»¶")

print("âœ… æ”¹å–„ç‰ˆã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆé–¢æ•°å®šç¾©å®Œäº†ï¼ˆAPIåˆ¶é™å¯¾ç­–ä»˜ãï¼‰")
print("ä½¿ç”¨æ–¹æ³•: export_unknown_contacts_to_gsheet_improved(contact_url_filled_df, failure_storage_SPREADSHEET_ID, 'å•ã„åˆã‚ã›URLæœªå–å¾—')")

# %% [markdown] id="twlHm1UABx98"
#

# %% [markdown] id="jivy57lYhj0k"
# # å–¶æ¥­æ–‡ç« ç”Ÿæˆ

# %% [markdown] id="vAuEPnLZFE-h"
# ## æº–å‚™

# %% id="dXNYHOYfekro"
import pandas as pd
from datetime import date

# %% [markdown] id="ora6nrXgZ7fM"
# ## ä¼šç¤¾ã®æƒ…å ±å–å¾—
#

# %% id="RehXryURvVyo"
import re, json

def classify_business_details(api_key: str, hp_url: str,
                              model: str="gemini-2.0-flash",
                              temperature: float=0.0,
                              timeout: int=300) -> dict:
    # â† ã“ã“ã§ timeout ã‚’ã¾ã¨ã‚ã¦åŠ¹ã‹ã›ã‚‹ï¼ˆrequest_options ã¯ä½¿ã‚ãªã„ï¼‰
    client = genai.Client(api_key=api_key, http_options={'api_version': 'v1alpha'})

    prompt = PROMPT_CLASSIFY.format(hp_url=hp_url,vocab_list=", ".join(BUSINESS_TYPE_VOCAB))
    # print("="*80)
    # print(prompt)
    # print("="*80)

    system_instruction=(
        "å…¬å¼ã‚µã‚¤ãƒˆã®ä¸€æ¬¡æƒ…å ±ã®ã¿ã‚’æ ¹æ‹ ã«æŠ½å‡ºã—ã€"
        "ä»¥ä¸‹ã‚­ãƒ¼ã ã‘ã®JSONæ–‡å­—åˆ—ã‚’è¿”ã™ã€‚ä½™è¨ˆãªæ–‡ã‚„ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã¯ä¸€åˆ‡ç¦æ­¢ï¼š"
        "company_display_name, business_type, other_label, strengths, values, "
        "address_text, evidence, confidence"
    ),

    # ãƒ„ãƒ¼ãƒ«ä½µç”¨æ™‚ã¯ response_mime_type/response_schema ã¯ä»˜ã‘ãªã„ï¼ˆ400å›é¿ï¼‰

    # æ¤œç´¢ãƒ„ãƒ¼ãƒ«ã‚’æœ‰åŠ¹åŒ–ï¼ˆã‚ãªãŸã®ä¾‹ã¨åŒã˜æ›¸å¼ï¼‰
    config = gtypes.GenerateContentConfig(
        system_instruction=system_instruction,
        tools=[{"google_search": {}}],
        temperature=temperature,
    )

    # å®Ÿè¡Œ
    resp = client.models.generate_content(
        model=model,
        config=types.GenerateContentConfig(
            system_instruction=system_instruction,
            tools=[{"google_search": {}}],
            temperature=temperature,
        ),
        contents=prompt
    )

    raw = (resp.text or "").strip()
    raw = re.sub(r"^```json\s*|\s*```$", "", raw).strip()
    return raw



# %% [markdown] id="Yf0DdHRe920T"
# #### ãƒŸãƒ‹ãƒ†ã‚¹ãƒˆ

# %% colab={"base_uri": "https://localhost:8080/"} id="2ZAAuEpGqG-t" outputId="0a68d9cc-cf31-47b4-fc43-68806e823007"
urls = [
    # "https://www.2and4-auto.jp/",
    # "https://www.cardrshuu.com/#contact",
    # "http://www.nomiyama-car.com/contact/",
    # "https://h-bpc.com/",
    # "https://tauros-japan.com/",
    # "https://www.saitomotor-hirosaki.com/",
    # "https://kurumayakoubou.jp/",
]
for url in urls:
  prompt = PROMPT_CLASSIFY.format(hp_url=url,vocab_list=", ".join(BUSINESS_TYPE_VOCAB))
  for i in range(1):
    resp = openai.responses.create(
      model="gpt-5-mini",  # ä¾‹ï¼šã‚³ã‚¹ãƒˆé‡è¦–ãªã‚‰ mini / å“è³ªé‡è¦–ãªã‚‰ gpt-5
      input=prompt,
      tools=[{"type": "web_search"}]  # å†…è”µWebæ¤œç´¢ã‚’æœ‰åŠ¹åŒ–
    )
    print(resp.output_text)
  print("="*80)

# %% [markdown] id="LZWwGZ-C986o"
# ## å–¶æ¥­æ–‡ç« ä½œæˆ

# %% id="BbghYR8v14iG"
# pip install openai  # æœªå°å…¥ãªã‚‰
import os, re
from openai import OpenAI

def generate_sales_copy_with_infomation(
    company_info: dict,
    prompt_template: str,
    *,
    model: str = "gpt-5-mini",   # ä¾‹: GPT-5 mini ç³»ã€‚ç’°å¢ƒã«åˆã‚ã›ã¦å¤‰æ›´å¯
    temperature: float = 1.0,
    timeout: int = 120,
    api_key: str | None = None,
) -> str:
    """
    Webæ¤œç´¢ã¯ä½¿ã‚ãšã€ä¸ãˆã‚‰ã‚ŒãŸä¼šç¤¾æƒ…å ±ã¨ãƒ†ãƒ³ãƒ—ãƒ¬ã‹ã‚‰å–¶æ¥­æ–‡ç« ã‚’ç”Ÿæˆã™ã‚‹ã€‚
    - company_info: {
        "company_name", "business_type", "other_label", "strengths",
        "values", "address_text", "evidence", "confidence"
      }
    - prompt_template: ä¾‹ç¤ºã®å–¶æ¥­ãƒ†ãƒ³ãƒ—ãƒ¬ï¼ˆ{business_type} ç­‰ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ã‚’å«ã‚€ï¼‰
    æˆ»ã‚Šå€¤: æ—¥æœ¬èªã®å–¶æ¥­æ–‡ç« ï¼ˆæ”¹è¡Œç¶­æŒï¼‰
    """

    # 1) ä¼šç¤¾æƒ…å ±ã®å‰å‡¦ç†ï¼ˆæ¬ æãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    company_name   = (company_info.get("company_name") or "").strip() or "è²´ç¤¾"
    business_type  = (company_info.get("business_type") or "").strip() or "ãã®ä»–"
    other_label    = (company_info.get("other_label") or "").strip()
    strengths      = (company_info.get("strengths") or "").strip()
    values         = (company_info.get("values") or "").strip()
    address_text   = (company_info.get("address_text") or "").strip()

    # "ãã®ä»–" ã¯ other_labelâ†’ãªã‘ã‚Œã°æ±ç§°
    bt_final = business_type if business_type != "ãã®ä»–" else (other_label or "åº—èˆ—")

    # 2) OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
    client = OpenAI(api_key=api_key or os.getenv("OPENAI_API_KEY", ""))

    # 3) ä¼šè©±è¨­å®šï¼ˆæ•¬æ„å¥ã¯å®šå‹ã«ã›ãšè‡ªç„¶æ–‡ã§ï¼‰
    system_msg = (
        "ã‚ãªãŸã¯æ—¥æœ¬èªã®B2Bå–¶æ¥­ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚"
        "å…¥åŠ›ã¨ã—ã¦ä¼šç¤¾æƒ…å ±ï¼ˆä¸€æ¬¡æƒ…å ±ç”±æ¥ã®è¦ç´„ï¼‰ã¨å–¶æ¥­ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒä¸ãˆã‚‰ã‚Œã¾ã™ã€‚"
        "å–¶æ¥­ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¯å®Œå…¨ã«ãƒ•ã‚©ãƒ­ãƒ¼ã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ã‚ˆã‚Šè‡ªç„¶ãªæ—¥æœ¬èªã«ã—ã¦ãã ã•ã„ã€‚"
        "ãŸã ã—ã€å–¶æ¥­ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®æœ€å¾Œã«ã‚ã‚‹ã€ã“ã¡ã‚‰ã®é€£çµ¡æƒ…å ±ã¯å¿…ãšæ­£ç¢ºã«éä¸è¶³ãªãåæ˜ ã•ã›ã¦ãã ã•ã„ã€‚"
        "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ã‚’åŸ‹ã‚ã€æ‹¬å¼§å†…ã®æŒ‡ç¤ºéƒ¨åˆ†ã¯äº‹å®Ÿãƒ™ãƒ¼ã‚¹ã®è‡ªç„¶ãªä¸€æ–‡ã§ç½®æ›ã—ã¦ãã ã•ã„ã€‚"
        "å¼•ç”¨ç•ªå·ã‚„ç”Ÿã®URLãƒªãƒ³ã‚¯ã¯æœ¬æ–‡ã«å…¥ã‚Œãªã„ã§ãã ã•ã„ï¼ˆç½²åæ¬„ã«å«ã¾ã‚Œã‚‹å›ºå®šURLã¯å¯ï¼‰ã€‚"
        "æ–‡ä½“ã¯ä¸å¯§ã€èª‡å¼µã¯é¿ã‘ã€æ”¹è¡Œãƒ»æ®µè½æ§‹æˆã¯ä¿ã£ã¦ãã ã•ã„ã€‚"
    )

    # 4) ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ï¼šä¼šç¤¾æƒ…å ±ï¼‹ãƒ†ãƒ³ãƒ—ãƒ¬
    user_msg = f"""
# ä¼šç¤¾æƒ…å ±ï¼ˆä¸€æ¬¡æƒ…å ±ã®è¦ç´„ï¼‰
company_name: {company_name}
business_type: {bt_final}
strengths: {strengths}
values: {values}
address_text: {address_text}

# é‡è¦ãªãƒ«ãƒ¼ãƒ«
- business_type ã¯ãã®ã¾ã¾ã€Œ{{business_type}}ã€ã¸å·®ã—è¾¼ã¿ï¼ˆè¨€ã„æ›ãˆä¸å¯ï¼‰ã€‚
- ã€Œï¼ˆã“ã“ã«{{strengths}}ã‚„{{values}}ã‚ˆã‚Šã€ã“ã®äº‹æ¥­æ‰€ã‚’ç§°ãˆã‚‹æ–‡ç« ã‚’ã„ã‚Œã¦ï¼‰ã€ã®éƒ¨åˆ†ã¯ã€
  strengths/values ã‹ã‚‰èª­ã¿å–ã‚Œã‚‹å…·ä½“ã‚’1ã€œ2ç‚¹ã ã‘ç¹”ã‚Šè¾¼ã‚“ã **è‡ªç„¶ãª1æ–‡**ã§ç½®æ›ã™ã‚‹ã“ã¨ã€‚
- å¼•ç”¨ç•ªå·ã‚„URLãƒªãƒ³ã‚¯ã¯æœ¬æ–‡ã«å…¥ã‚Œãªã„ã€‚
- å‡ºåŠ›ã¯æœ¬æ–‡ã®ã¿ï¼ˆã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ç¦æ­¢ï¼‰ã€‚

# å–¶æ¥­ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        {
            prompt_template
        #  .format(
        #     business_type=bt_final,
        #     company_name=company_name,
        #     strengths=strengths or "ï¼ˆå¼·ã¿æƒ…å ±ã¯æœªå–å¾—ï¼‰",
        #     values=values or "ï¼ˆç†å¿µæƒ…å ±ã¯æœªå–å¾—ï¼‰",
        #     address_text=address_text)
        }
    """.strip()

    # print("prompt:","="*80)
    # print(user_msg)
    # print("="*80)

    resp = client.chat.completions.create(
        model=model,
        messages=[
            {"role":"system", "content": system_msg},
            {"role":"user",   "content": user_msg},
        ],
        temperature=temperature,
        timeout=timeout,
    )

    text = resp.choices[0].message.content.strip()

    # ãƒ•ã‚§ãƒ³ã‚¹é™¤å» & ä½™è¨ˆãªã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å¯¾ç­–
    text = re.sub(r"^```(?:\w+)?\s*|\s*```$", "", text, flags=re.S).strip()
    return text


# %%
import sys
print("Python executable:", sys.executable)
print("Python version:", sys.version)
print("Python path:", sys.path[:3])  # æœ€åˆã®3ã¤ã®ãƒ‘ã‚¹ã‚’è¡¨ç¤º

# %% [markdown] id="KUT2pDUh-PoX"
# #### ãƒ†ã‚¹ãƒˆ

# %% colab={"base_uri": "https://localhost:8080/"} id="QuL4XRFB6Dbf" outputId="dbfda394-5be8-4f7e-a155-1e4232a0ef7d"
import json
urls = [
    # "https://www.2and4-auto.jp/",
    # "https://www.cardrshuu.com/#contact",
    # "http://www.nomiyama-car.com/contact/",
    # "https://h-bpc.com/",
    # "https://tauros-japan.com/",
    # "https://www.saitomotor-hirosaki.com/",
    # "https://kurumayakoubou.jp/",
]
for url in urls:
  resp = openai.responses.create(
    model="gpt-5-mini",  # ä¾‹ï¼šã‚³ã‚¹ãƒˆé‡è¦–ãªã‚‰ mini / å“è³ªé‡è¦–ãªã‚‰ gpt-5
    input=PROMPT_CLASSIFY.format(hp_url=url,vocab_list=", ".join(BUSINESS_TYPE_VOCAB)),
    tools=[{"type": "web_search"}]  # å†…è”µWebæ¤œç´¢ã‚’æœ‰åŠ¹åŒ–
  )
  comp_data = json.loads(resp.output_text)
  text = generate_sales_copy_with_infomation(comp_data, PROMPT, model="gpt-5-mini")
  print(text)
  # print("="*80)

# %% [markdown] id="bzGD3GSP6DJM"
# # å®Ÿè¡Œãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

# %% id="LN7rLf5OHTCO"
# äº‹å‰: !pip -q install tqdm openai
import json, re, time
from typing import Iterable, Optional
import pandas as pd
from openai import OpenAI
from tqdm.auto import tqdm

def _extract_json(text: str) -> str:
    s = text.strip()
    s = re.sub(r"^```(?:json)?\s*|\s*```$", "", s, flags=re.S).strip()
    m = re.search(r"\{[\s\S]*\}$", s)
    return (m.group(0) if m else s)

def fill_sales_copy_with_gpt(
    df: pd.DataFrame,
    *,
    url_col: str = "hp_url",
    out_col: str = "sales_copy",
    model: str = "gpt-5-mini",
    classify_prompt_template: str = None,   # ä¾‹: PROMPT_CLASSIFY
    sales_prompt_template: str = None,      # ä¾‹: PROMPT
    business_vocab: Optional[Iterable[str]] = None,  # ä¾‹: BUSINESS_TYPE_VOCAB
    overwrite: bool = True,
    sleep_sec: float = 0.8,
    openai_api_key: Optional[str] = None,
) -> pd.DataFrame:
    """
    å„è¡Œ: hp_url -> (åˆ†é¡JSON) -> generate_sales_copy_with_infomation -> sales_copy ã«æ ¼ç´
    é€²æ—ãƒãƒ¼ã¯1æœ¬ã ã‘è¡¨ç¤ºã€‚
    """
    if out_col not in df.columns:
        df.loc[:, out_col] = ""

    if classify_prompt_template is None or sales_prompt_template is None or business_vocab is None:
        raise ValueError("classify_prompt_template / sales_prompt_template / business_vocab ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")

    client = OpenAI(api_key=openai_api_key or os.getenv("OPENAI_API_KEY", ""))
    mask = df[url_col].notna() & df[url_col].astype(str).str.strip().ne("")
    idxs = df[mask].index
    vocab_str = ", ".join(business_vocab)

    # --- é€²æ—ãƒãƒ¼ ---
    for i in tqdm(idxs, total=len(idxs), desc="å–¶æ¥­æ–‡ç”Ÿæˆ", unit="ç¤¾"):
        if (not overwrite) and isinstance(df.at[i, out_col], str) and df.at[i, out_col].strip():
            continue

        url = str(df.at[i, url_col]).strip()
        try:
            # 1) åˆ†é¡ï¼ˆJSONç”Ÿæˆãƒ»Webæ¤œç´¢ONï¼‰
            prompt_cls = classify_prompt_template.format(hp_url=url, vocab_list=vocab_str)
            # print("reach here ==============")
            resp = client.responses.create(
                model=model,
                input=prompt_cls,
                tools=[{"type": "web_search"}],
            )
            comp_json = _extract_json(resp.output_text)
            comp_data = json.loads(comp_json)
            # print("reach fetching comp data ==============")

            # 2) å–¶æ¥­æ–‡ç”Ÿæˆï¼ˆæ¤œç´¢ãªã—ï¼‰
            text = generate_sales_copy_with_infomation(
                company_info=comp_data,
                prompt_template=sales_prompt_template,
                model=model,
                temperature=1.0,
            )
            # print("text",text)
            df.at[i, out_col] = (text or "").strip()

        except Exception:
            df.at[i, out_col] = ""
        time.sleep(sleep_sec)

    return df
