# ---
# jupyter:
#   jupytext:
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.17.3
#   kernelspec:
#     display_name: base
#     language: python
#     name: python3
# ---

# %% [markdown] id="8RnUTOYIaFBb"
# # äº‹å‰è¨­å®š

# %% colab={"base_uri": "https://localhost:8080/"} id="ptwJFOn7p4DD" outputId="47bde079-b9f8-48eb-c130-0a0339c0aa49"
# â”€â”€â”€ å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# !pip install google-api-python-client beautifulsoup4
# !pip -q install openpyxl odfpy
# !pip -q install gspread gspread_dataframe

# %% id="4y2w5yroWJuW"
import os
from pathlib import Path
from dotenv import load_dotenv, find_dotenv
from google.cloud import bigquery
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç›´ä¸‹ã® .env ã‚’ç‰¹å®šã—ã¦ä¸Šæ›¸ãèª­ã¿è¾¼ã¿
project_root = next(p for p in [Path.cwd(), *Path.cwd().parents] if (p / ".env").exists())
env_file = str(project_root / ".env")
load_dotenv(dotenv_path=env_file, override=True)

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", "")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "")
CSE_ID = os.getenv("CSE_ID", "")
GCLOUD_PROJECT_ID = os.getenv("GClOUD_PROJECT_ID", "")
print("GCLOUD_PROJECT_ID:",GCLOUD_PROJECT_ID)
import os

# %%
import sys
print(sys.executable)

# %% [markdown]
# # å•ã„åˆã‚ã›URLå–å¾—

# %% [markdown]
# ## å•ã„åˆã‚ã›å–å¾—

# %% id="j9Q1McZspUh7"
# â”€â”€â”€ ã‚¤ãƒ³ãƒãƒ¼ãƒˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import pandas as pd
import requests
import logging
import time
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from googleapiclient.discovery import build
from typing import Optional, Tuple, List
from datetime import datetime
from tqdm.auto import tqdm
from datetime import datetime

# â”€â”€â”€ ãƒ­ã‚®ãƒ³ã‚°è¨­å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(level=logging.DEBUG,
                    format='%(asctime)s %(levelname)s %(message)s')

# â”€â”€â”€ Google Custom Search API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä½œæˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def make_search_client(api_key: str, cse_id: str):
    logging.debug("Creating Custom Search client")
    client = build("customsearch", "v1", developerKey=api_key).cse()
    logging.debug("Custom Search client created")
    return client

# â”€â”€â”€ ä¼šç¤¾åã§å…¬å¼ã‚µã‚¤ãƒˆ URL ã‚’å–å¾— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_hp_url(company_name: str, cse_client, cse_id: str) -> str:
    logging.debug(f"Searching HP URL for: {company_name}")
    try:
        res = cse_client.list(q=company_name, cx=cse_id, num=1).execute()
        items = res.get("items", [])
        hp = items[0]["link"] if items else None
        logging.debug(f"â†’ HP URL found: {hp}")
        return hp
    except Exception as e:
        logging.error(f"Error fetching HP URL for {company_name}: {e}")
        return None

# â”€â”€â”€ ãƒšãƒ¼ã‚¸ãŒãƒ•ã‚©ãƒ¼ãƒ ã‹åˆ¤å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def is_form_page(soup: BeautifulSoup) -> bool:

    text_types ={"text","email","tel","number"}
    inputs = soup.find_all(
        "input",
        attrs={
            "type": lambda t: t and t.lower() in text_types,
            "name": True
        }
    )
    return len(inputs) >= 3

    return True

# â”€â”€â”€ å•ã„åˆã›URLå–å¾— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_contact_url(hp_url: str, timeout: float = 5.0) -> Optional[str]:
    """
    hp_url ã‹ã‚‰æœ€å¤§æ·±åº¦3ã¾ã§ãƒªãƒ³ã‚¯ã‚’ãŸã©ã‚Šã€
    ãŠå•ã„åˆã‚ã›ãƒ•ã‚©ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ã¨åˆ¤æ–­ã§ããŸ URL ã‚’è¿”ã™ã€‚
    è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã° Noneã€‚
    """
    logging.debug(f"Searching contact URL on: {hp_url}")
    if not hp_url:
        logging.warning("No HP URL provided, skipping contact search")
        return None

    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å®šç¾©
    primary_kw   : List[str] = ["å•ã„åˆã‚ã›", "ãŠå•ã„åˆã‚ã›", "å•åˆã‚ã›", "å•ã„åˆã›", "ã‚³ãƒ³ã‚¿ã‚¯ãƒˆ", "contact", "inquiry", "request", "entry"]
    secondary_kw : List[str] = ["ãƒ•ã‚©ãƒ¼ãƒ ", "ãã®ä»–", "æ¡ç”¨", "IR", "æœ¬éƒ¨"] + primary_kw

    session = requests.Session()

    def fetch_soup(url: str) -> Tuple[Optional[BeautifulSoup], Optional[str]]:
        """ URL ã‚’ GET ã—ã¦ (soup, æœ€çµ‚çš„ãªçµ¶å¯¾URL) ã‚’è¿”ã™ """
        try:
            resp = session.get(url, timeout=timeout)
            resp.raise_for_status()
            return BeautifulSoup(resp.text, "html.parser"), resp.url
        except Exception as e:
            # logging.warning(f"  â†’ Failed to fetch {url}: {e}")
            return None, None

    def extract_links(soup: BeautifulSoup, base_url: str, kws: List[str]) -> List[str]:
        """
        <a href> ã® text or href ã«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹ã‚‚ã®ã‚’æŠ½å‡ºã—ã€
        çµ¶å¯¾URLã§è¿”ã™
        """
        results: List[str] = []
        for a in soup.find_all("a", href=True):
            text = a.get_text(strip=True).lower()
            href = a["href"].lower()
            if any(kw in text for kw in kws) or any(kw in href for kw in kws):
                abs_url = urljoin(base_url, a["href"])
                logging.debug(f"   â†’ Candidate link: {abs_url}")
                results.append(abs_url)
        return results

    # æ·±åº¦ã”ã¨ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒ—
    kw_by_depth = {1: primary_kw, 2: secondary_kw, 3: secondary_kw}

    # BFS ã§æœ€å¤§æ·±åº¦3ã¾ã§æ¢ç´¢
    frontier = [(hp_url, 0)]  # (URL, depth)
    visited  = set()

    while frontier:
        url, depth = frontier.pop(0)
        if url in visited or depth >= 3:
            continue
        visited.add(url)

        soup, real_url = fetch_soup(url)
        if soup is None:
            continue

        logging.debug(f"[depth={depth}] Visiting: {real_url}")

        # depth>0 ã®ãƒšãƒ¼ã‚¸ã§ãƒ•ã‚©ãƒ¼ãƒ åˆ¤å®š
        if depth > 0 and is_form_page(soup):
            # logging.info(f"Contact form URL found at depth {depth}: {real_url}")
            # print(f"Contact form URL found at depth {depth}: {real_url}")
            return real_url

        # æ¬¡ã®æ·±åº¦ã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºã—ã¦ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
        next_depth = depth + 1
        kws = kw_by_depth.get(next_depth, [])
        if not kws:
            continue

        for link in extract_links(soup, real_url, kws):
            frontier.append((link, next_depth))

    logging.info("Contact form URL not found within depth 3")
    return None

def fill_contact_from_hp(df):
    mask = df['contact_url'].isna() & df['hp_url'].str.contains(r'contact|inquiry|toiawase|ãŠå•ã„åˆã‚ã›|ãŠå•åˆã›', case=False, na=False)
    df.loc[mask, 'contact_url'] = df.loc[mask, 'hp_url']
    return df


# â”€â”€â”€ DataFrame ã«å¯¾ã—ã¦ä¸€æ‹¬å‡¦ç† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fill_urls(df: pd.DataFrame, api_key: str, cse_id: str) -> pd.DataFrame:
    cse = make_search_client(api_key, cse_id)
    hp_urls = []
    contact_urls = []

    for i, name in tqdm(enumerate(df["company_name"], start=1)):
        # print(f"=== å‡¦ç†é–‹å§‹ {i}/{len(df)}: {name} ===")
        # logging.info(f"=== å‡¦ç†é–‹å§‹ {i}/{len(df)}: {name} ===")

        # å…¬å¼ã‚µã‚¤ãƒˆ URL
        hp = get_hp_url(name, cse, cse_id)
        hp_urls.append(hp)

        # å•ã„åˆã‚ã›ãƒ•ã‚©ãƒ¼ãƒ  URL
        contact = get_contact_url(hp)
        contact_urls.append(contact)

        logging.info(f"â†’ çµæœ: HP={hp}, Contact={contact}\n")

        time.sleep(1)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–

    df["hp_url"] = hp_urls
    df["contact_url"] = contact_urls
    return df


# %%
from dotenv import load_dotenv
load_dotenv()  # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç›´ä¸‹ã® .env ã‚’èª­ã¿è¾¼ã‚€

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", "")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "")
CSE_ID = os.getenv("CSE_ID", "")
cse = make_search_client(GOOGLE_API_KEY, CSE_ID)
print("google_api_key:",GOOGLE_API_KEY)
cse = make_search_client(GOOGLE_API_KEY, CSE_ID)

# %% [markdown] id="hAK_42aaWSrB"
# ## ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆä¸Šä¸æ˜ã®ã‚‚ã®ã‚’ä¿å­˜

# %% id="ckdsDSCmjO8K"
# è¿½åŠ : ãƒãƒƒã‚·ãƒ¥ä»˜ãã‚¢ãƒ³ã‚«ãƒ¼ã‚‚å€™è£œã«å…¥ã‚Œã‚‹
COMMON_RELATIVE_PATHS = [
    "/contact",
    "/contact-us",
    "/contacact.html",
    "/contact/other/",
    "/contact/others/",
    "/contact/form",
    "/contact/recruit/",
    "/inquiry",
    "/inquiries",
    "/request",
    "/requests",
    "#contact",            # â† ã“ã‚Œã‚’è¿½åŠ 
]

from urllib.parse import urljoin
from bs4 import BeautifulSoup
import requests
import pandas as pd
import re
from tqdm import tqdm

def fill_contact_url(df: pd.DataFrame, timeout: float = 7.0) -> pd.DataFrame:
    null_mask = df["contact_url"].isna() | df["contact_url"].astype(str).str.strip().eq("")
    print(null_mask)
    for idx, row in tqdm(df[null_mask].iterrows(), total=null_mask.sum()):
        base_url = row.get("hp_url")
        if not base_url:
            continue
        # ãƒ™ãƒ¼ã‚¹ãƒšãƒ¼ã‚¸ã¯1å›ã ã‘å–å¾—ï¼ˆ#fragmentç”¨ï¼‰
        soup_home = None
        try:
            res_home = requests.get(base_url, timeout=timeout)
            if res_home.status_code == 200:
                soup_home = BeautifulSoup(res_home.content, "html.parser")
        except Exception as e:
            print(f"Error fetching base page {base_url}: {e}")

        tested = set()
        found = False

        for path in COMMON_RELATIVE_PATHS:
            # ãƒãƒƒã‚·ãƒ¥ï¼ˆ#...ï¼‰ã¯ãƒšãƒ¼ã‚¸å†…ã‚¢ãƒ³ã‚«ãƒ¼æ‰±ã„
            if path.startswith("#"):
                if soup_home is None:
                    continue  # ãƒ›ãƒ¼ãƒ å–å¾—å¤±æ•—æ™‚ã¯ã‚¹ã‚­ãƒƒãƒ—
                # è©²å½“ã‚¢ãƒ³ã‚«ãƒ¼ãŒå­˜åœ¨ã™ã‚‹ã‹
                target = soup_home.select_one(path)  # ä¾‹: '#contact'
                if not target:
                    continue

                # ã‚»ã‚¯ã‚·ãƒ§ãƒ³å˜ä½ã§ãƒ•ã‚©ãƒ¼ãƒ ã‚‰ã—ã•ã‚’åˆ¤å®šï¼ˆã ã‚ãªã‚‰ãƒšãƒ¼ã‚¸å…¨ä½“ã§åˆ¤å®šï¼‰
                try:
                    if is_form_page(target) or is_form_page(soup_home):
                        contact_url = urljoin(base_url, path)
                        df.at[idx, "contact_url"] = contact_url
                        print(f"Found contact URL (fragment): {contact_url}")
                        found = True
                        break
                except Exception as e:
                    # is_form_page ãŒ Tag ã‚’æƒ³å®šã—ã¦ã„ãªã„å ´åˆã¯ãƒšãƒ¼ã‚¸å…¨ä½“ã§å†åˆ¤å®š
                    try:
                        if is_form_page(soup_home):
                            contact_url = urljoin(base_url, path)
                            df.at[idx, "contact_url"] = contact_url
                            print(f"Found contact URL (fragment-fallback): {contact_url}")
                            found = True
                            break
                    except Exception as ee:
                        print(f"is_form_page error on fragment {path}: {ee}")
                continue

            # é€šå¸¸ã®ç›¸å¯¾ãƒ‘ã‚¹ã¯ä»Šã¾ã§é€šã‚Šå–å¾—ã—ã¦åˆ¤å®š
            test_url = urljoin(base_url, path)
            if test_url in tested:
                continue
            tested.add(test_url)

            try:
                res = requests.get(test_url, timeout=timeout)
                if res.status_code == 200:
                    soup = BeautifulSoup(res.content, "html.parser")
                    if is_form_page(soup):
                        df.at[idx, "contact_url"] = test_url
                        print(f"Found contact URL: {test_url}")
                        found = True
                        break  # æœ€åˆã«è¦‹ã¤ã‘ãŸãƒ•ã‚©ãƒ¼ãƒ ã§çµ‚äº†
            except Exception as e:
                print(f"Error fetching {test_url}: {e}")
                continue  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚„æ¥ç¶šã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–

        # è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ã€ãƒ›ãƒ¼ãƒ å†…ã®<a href="#...">ã‹ã‚‰#contactç³»ã‚’è£œè¶³ï¼ˆä¿é™ºï¼‰
        if not found and soup_home is not None:
            try:
                anchors = soup_home.select('a[href^="#"]')
                for a in anchors:
                    href = a.get("href", "")
                    text = (a.get_text() or "") + " " + href
                    if re.search(r"(contact|inquiry|ãŠå•ã„åˆã‚ã›|ãŠå•åˆã›|å•åˆã›)", text, re.I):
                        target = soup_home.select_one(href)
                        if target and (is_form_page(target) or is_form_page(soup_home)):
                            contact_url = urljoin(base_url, href)
                            df.at[idx, "contact_url"] = contact_url
                            print(f"Found contact URL (fragment-auto): {contact_url}")
                            break
            except Exception as e:
                print(f"Error scanning fragments on {base_url}: {e}")

    return df


# %% id="n2bNifpYYm0a"
# === æ”¹å–„ç‰ˆã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆé–¢æ•°ï¼ˆAPIåˆ¶é™å¯¾ç­–ä»˜ãï¼‰ ===
def export_unknown_contacts_to_gsheet_improved(df, spreadsheet_id, sheet_name):
    """
    æ”¹å–„ç‰ˆï¼šå•ã„åˆã‚ã›URLãŒæœªå–å¾—ã®ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã‚’Google Sheetsã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹
    ï¼ˆæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒã—ã¦æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ã€APIåˆ¶é™å¯¾ç­–ä»˜ãï¼‰
    """
    try:
        import gspread
        from gspread_dataframe import set_with_dataframe
        from google.oauth2.service_account import Credentials
        import os
        import time
        
        # èªè¨¼æƒ…å ±ã®è¨­å®š
        scope = [
            'https://spreadsheets.google.com/feeds',
            'https://www.googleapis.com/auth/drive',
            'https://www.googleapis.com/auth/spreadsheets'
        ]
        
        # ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’ç¢ºèªï¼ˆVMå¯¾å¿œï¼‰
        project_root = get_form_sales_root()
        service_account_key_path = project_root / "secrets" / "form-sales-log-bffd68dc6996.json"
        
        if not service_account_key_path.exists():
            print(f"âŒ ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {service_account_key_path}")
            return export_unknown_contacts_to_csv(df)
        
        # ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
        credentials = Credentials.from_service_account_file(
            str(service_account_key_path), 
            scopes=scope
        )
        gc = gspread.authorize(credentials)
        
        print(f"âœ… èªè¨¼æˆåŠŸ: {credentials.service_account_email}")
        
        # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚’é–‹ã
        try:
            spreadsheet = gc.open_by_key(spreadsheet_id)
            print(f"âœ… ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚¢ã‚¯ã‚»ã‚¹æˆåŠŸ: {spreadsheet.title}")
        except Exception as e:
            print(f"âŒ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
            return export_unknown_contacts_to_csv(df)
        
        # ã‚·ãƒ¼ãƒˆã‚’å–å¾—ã¾ãŸã¯ä½œæˆ
        try:
            worksheet = spreadsheet.worksheet(sheet_name)
            print(f"âœ… æ—¢å­˜ã‚·ãƒ¼ãƒˆã‚’ä½¿ç”¨: {sheet_name}")
        except gspread.WorksheetNotFound:
            worksheet = spreadsheet.add_worksheet(title=sheet_name, rows=1000, cols=20)
            print(f"âœ… æ–°è¦ã‚·ãƒ¼ãƒˆã‚’ä½œæˆ: {sheet_name}")
        
        # å•ã„åˆã‚ã›URLãŒæœªå–å¾—ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        unknown_contacts = df[
            df["contact_url"].isna() | 
            (df["contact_url"].str.strip() == "") |
            (df["contact_url"] == "None")
        ].copy()
        
        if len(unknown_contacts) == 0:
            print("âœ… ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹æœªå–å¾—ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“")
            return
        
        print(f"ğŸ“Š ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå¯¾è±¡: {len(unknown_contacts)}ä»¶")
        
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
        existing_data = worksheet.get_all_values()
        print(f"ğŸ“‹ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿è¡Œæ•°: {len(existing_data)}")
        
        # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ï¼ˆæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒï¼‰
        if len(existing_data) == 0:
            # ã‚·ãƒ¼ãƒˆãŒç©ºã®å ´åˆã€ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
            print("ï¿½ï¿½ ç©ºã®ã‚·ãƒ¼ãƒˆã«ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ã—ã¾ã™")
            set_with_dataframe(worksheet, unknown_contacts)
        else:
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã€ãƒãƒƒãƒå‡¦ç†ã§ä¸€æ‹¬è¿½åŠ ï¼ˆAPIåˆ¶é™å¯¾ç­–ï¼‰
            print("ï¿½ï¿½ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®æœ€çµ‚è¡Œã‹ã‚‰æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ‹¬è¿½åŠ ã—ã¾ã™")
            
            # æœ€çµ‚è¡Œã®è¡Œç•ªå·ã‚’å–å¾—
            next_row = len(existing_data) + 1
            
            # ãƒãƒƒãƒå‡¦ç†ã§ä¸€æ‹¬è¿½åŠ ï¼ˆAPIåˆ¶é™å¯¾ç­–ï¼‰
            batch_data = []
            for row in unknown_contacts.values:
                # ãƒ‡ãƒ¼ã‚¿ã‚’æ–‡å­—åˆ—ã¨ã—ã¦å¤‰æ›ï¼ˆNoneã‚’ç©ºæ–‡å­—åˆ—ã«ï¼‰
                row_data = [str(val) if val is not None else "" for val in row]
                batch_data.append(row_data)
            
            # ä¸€æ‹¬ã§ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
            worksheet.update(f'A{next_row}', batch_data)
            
            # APIåˆ¶é™å¯¾ç­–ã®ãŸã‚å°‘ã—å¾…æ©Ÿ
            time.sleep(2)
            
            print(f"âœ… {len(unknown_contacts)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’{next_row}è¡Œç›®ã‹ã‚‰ä¸€æ‹¬è¿½åŠ ã—ã¾ã—ãŸ")
        
        print(f"ğŸ‰ å®Œäº†: {len(unknown_contacts)}ä»¶ã®æœªå–å¾—ãƒ‡ãƒ¼ã‚¿ã‚’{sheet_name}ã‚·ãƒ¼ãƒˆã«è¿½åŠ ã—ã¾ã—ãŸ")
        
    except Exception as e:
        print(f"âŒ Google Sheetsã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        print("CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¾ã™...")
        export_unknown_contacts_to_csv(df)

def export_unknown_contacts_to_csv(df, filename=None):
    """
    CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹é–¢æ•°ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰
    """
    if filename is None:
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"unknown_contacts_{timestamp}.csv"
    
    # å•ã„åˆã‚ã›URLãŒæœªå–å¾—ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    unknown_contacts = df[
        df["contact_url"].isna() | 
        (df["contact_url"].str.strip() == "") |
        (df["contact_url"] == "None")
    ].copy()
    
    if len(unknown_contacts) == 0:
        print("âœ… ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹æœªå–å¾—ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“")
        return
    
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    unknown_contacts.to_csv(filename, index=False, encoding='utf-8-sig')
    print(f"ğŸ“ CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã—ãŸ: {filename}")
    print(f"ï¿½ï¿½ ä¿å­˜ä»¶æ•°: {len(unknown_contacts)}ä»¶")

print("âœ… æ”¹å–„ç‰ˆã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆé–¢æ•°å®šç¾©å®Œäº†ï¼ˆAPIåˆ¶é™å¯¾ç­–ä»˜ãï¼‰")
print("ä½¿ç”¨æ–¹æ³•: export_unknown_contacts_to_gsheet_improved(contact_url_filled_df, failure_storage_SPREADSHEET_ID, 'å•ã„åˆã‚ã›URLæœªå–å¾—')")

# %% [markdown] id="twlHm1UABx98"
#

# %% [markdown] id="jivy57lYhj0k"
# # å–¶æ¥­æ–‡ç« ç”Ÿæˆ

# %% [markdown] id="vAuEPnLZFE-h"
# ## æº–å‚™

# %% id="dXNYHOYfekro"
import pandas as pd
from datetime import date

# %% [markdown] id="ora6nrXgZ7fM"
# ## ä¼šç¤¾ã®æƒ…å ±å–å¾—
#

# %% id="RehXryURvVyo"
import re, json

def classify_business_details(api_key: str, hp_url: str,
                              model: str="gemini-2.0-flash",
                              temperature: float=0.0,
                              timeout: int=300) -> dict:
    # â† ã“ã“ã§ timeout ã‚’ã¾ã¨ã‚ã¦åŠ¹ã‹ã›ã‚‹ï¼ˆrequest_options ã¯ä½¿ã‚ãªã„ï¼‰
    client = genai.Client(api_key=api_key, http_options={'api_version': 'v1alpha'})

    prompt = PROMPT_CLASSIFY.format(hp_url=hp_url,vocab_list=", ".join(BUSINESS_TYPE_VOCAB))
    # print("="*80)
    # print(prompt)
    # print("="*80)

    system_instruction=(
        "å…¬å¼ã‚µã‚¤ãƒˆã®ä¸€æ¬¡æƒ…å ±ã®ã¿ã‚’æ ¹æ‹ ã«æŠ½å‡ºã—ã€"
        "ä»¥ä¸‹ã‚­ãƒ¼ã ã‘ã®JSONæ–‡å­—åˆ—ã‚’è¿”ã™ã€‚ä½™è¨ˆãªæ–‡ã‚„ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã¯ä¸€åˆ‡ç¦æ­¢ï¼š"
        "company_display_name, business_type, other_label, strengths, values, "
        "address_text, evidence, confidence"
    ),

    # ãƒ„ãƒ¼ãƒ«ä½µç”¨æ™‚ã¯ response_mime_type/response_schema ã¯ä»˜ã‘ãªã„ï¼ˆ400å›é¿ï¼‰

    # æ¤œç´¢ãƒ„ãƒ¼ãƒ«ã‚’æœ‰åŠ¹åŒ–ï¼ˆã‚ãªãŸã®ä¾‹ã¨åŒã˜æ›¸å¼ï¼‰
    config = gtypes.GenerateContentConfig(
        system_instruction=system_instruction,
        tools=[{"google_search": {}}],
        temperature=temperature,
    )

    # å®Ÿè¡Œ
    resp = client.models.generate_content(
        model=model,
        config=types.GenerateContentConfig(
            system_instruction=system_instruction,
            tools=[{"google_search": {}}],
            temperature=temperature,
        ),
        contents=prompt
    )

    raw = (resp.text or "").strip()
    raw = re.sub(r"^```json\s*|\s*```$", "", raw).strip()
    return raw



# %% [markdown] id="Yf0DdHRe920T"
# #### ãƒŸãƒ‹ãƒ†ã‚¹ãƒˆ

# %% colab={"base_uri": "https://localhost:8080/"} id="2ZAAuEpGqG-t" outputId="0a68d9cc-cf31-47b4-fc43-68806e823007"
urls = [
    # "https://www.2and4-auto.jp/",
    # "https://www.cardrshuu.com/#contact",
    # "http://www.nomiyama-car.com/contact/",
    # "https://h-bpc.com/",
    # "https://tauros-japan.com/",
    # "https://www.saitomotor-hirosaki.com/",
    # "https://kurumayakoubou.jp/",
]
for url in urls:
  prompt = PROMPT_CLASSIFY.format(hp_url=url,vocab_list=", ".join(BUSINESS_TYPE_VOCAB))
  for i in range(1):
    resp = openai.responses.create(
      model="gpt-5-mini",  # ä¾‹ï¼šã‚³ã‚¹ãƒˆé‡è¦–ãªã‚‰ mini / å“è³ªé‡è¦–ãªã‚‰ gpt-5
      input=prompt,
      tools=[{"type": "web_search"}]  # å†…è”µWebæ¤œç´¢ã‚’æœ‰åŠ¹åŒ–
    )
    print(resp.output_text)
  print("="*80)

# %% [markdown] id="LZWwGZ-C986o"
# ## å–¶æ¥­æ–‡ç« ä½œæˆ

# %% id="BbghYR8v14iG"
# pip install openai  # æœªå°å…¥ãªã‚‰
import os, re
from openai import OpenAI

def generate_sales_copy_with_infomation(
    company_info: dict,
    prompt_template: str,
    *,
    model: str = "gpt-5-mini",   # ä¾‹: GPT-5 mini ç³»ã€‚ç’°å¢ƒã«åˆã‚ã›ã¦å¤‰æ›´å¯
    temperature: float = 1.0,
    timeout: int = 120,
    api_key: str | None = None,
) -> str:
    """
    Webæ¤œç´¢ã¯ä½¿ã‚ãšã€ä¸ãˆã‚‰ã‚ŒãŸä¼šç¤¾æƒ…å ±ã¨ãƒ†ãƒ³ãƒ—ãƒ¬ã‹ã‚‰å–¶æ¥­æ–‡ç« ã‚’ç”Ÿæˆã™ã‚‹ã€‚
    - company_info: {
        "company_name", "business_type", "other_label", "strengths",
        "values", "address_text", "evidence", "confidence"
      }
    - prompt_template: ä¾‹ç¤ºã®å–¶æ¥­ãƒ†ãƒ³ãƒ—ãƒ¬ï¼ˆ{business_type} ç­‰ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ã‚’å«ã‚€ï¼‰
    æˆ»ã‚Šå€¤: æ—¥æœ¬èªã®å–¶æ¥­æ–‡ç« ï¼ˆæ”¹è¡Œç¶­æŒï¼‰
    """

    # 1) ä¼šç¤¾æƒ…å ±ã®å‰å‡¦ç†ï¼ˆæ¬ æãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    company_name   = (company_info.get("company_name") or "").strip() or "è²´ç¤¾"
    business_type  = (company_info.get("business_type") or "").strip() or "ãã®ä»–"
    other_label    = (company_info.get("other_label") or "").strip()
    strengths      = (company_info.get("strengths") or "").strip()
    values         = (company_info.get("values") or "").strip()
    address_text   = (company_info.get("address_text") or "").strip()

    # "ãã®ä»–" ã¯ other_labelâ†’ãªã‘ã‚Œã°æ±ç§°
    bt_final = business_type if business_type != "ãã®ä»–" else (other_label or "åº—èˆ—")

    # 2) OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
    client = OpenAI(api_key=api_key or os.getenv("OPENAI_API_KEY", ""))

    # 3) ä¼šè©±è¨­å®šï¼ˆæ•¬æ„å¥ã¯å®šå‹ã«ã›ãšè‡ªç„¶æ–‡ã§ï¼‰
    system_msg = (
        "ã‚ãªãŸã¯æ—¥æœ¬èªã®B2Bå–¶æ¥­ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚"
        "å…¥åŠ›ã¨ã—ã¦ä¼šç¤¾æƒ…å ±ï¼ˆä¸€æ¬¡æƒ…å ±ç”±æ¥ã®è¦ç´„ï¼‰ã¨å–¶æ¥­ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒä¸ãˆã‚‰ã‚Œã¾ã™ã€‚"
        "å–¶æ¥­ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¯å®Œå…¨ã«ãƒ•ã‚©ãƒ­ãƒ¼ã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ã‚ˆã‚Šè‡ªç„¶ãªæ—¥æœ¬èªã«ã—ã¦ãã ã•ã„ã€‚"
        "ãŸã ã—ã€å–¶æ¥­ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®æœ€å¾Œã«ã‚ã‚‹ã€ã“ã¡ã‚‰ã®é€£çµ¡æƒ…å ±ã¯å¿…ãšæ­£ç¢ºã«éä¸è¶³ãªãåæ˜ ã•ã›ã¦ãã ã•ã„ã€‚"
        "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ã‚’åŸ‹ã‚ã€æ‹¬å¼§å†…ã®æŒ‡ç¤ºéƒ¨åˆ†ã¯äº‹å®Ÿãƒ™ãƒ¼ã‚¹ã®è‡ªç„¶ãªä¸€æ–‡ã§ç½®æ›ã—ã¦ãã ã•ã„ã€‚"
        "å¼•ç”¨ç•ªå·ã‚„ç”Ÿã®URLãƒªãƒ³ã‚¯ã¯æœ¬æ–‡ã«å…¥ã‚Œãªã„ã§ãã ã•ã„ï¼ˆç½²åæ¬„ã«å«ã¾ã‚Œã‚‹å›ºå®šURLã¯å¯ï¼‰ã€‚"
        "æ–‡ä½“ã¯ä¸å¯§ã€èª‡å¼µã¯é¿ã‘ã€æ”¹è¡Œãƒ»æ®µè½æ§‹æˆã¯ä¿ã£ã¦ãã ã•ã„ã€‚"
    )

    # 4) ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ï¼šä¼šç¤¾æƒ…å ±ï¼‹ãƒ†ãƒ³ãƒ—ãƒ¬
    user_msg = f"""
# ä¼šç¤¾æƒ…å ±ï¼ˆä¸€æ¬¡æƒ…å ±ã®è¦ç´„ï¼‰
company_name: {company_name}
business_type: {bt_final}
strengths: {strengths}
values: {values}
address_text: {address_text}

# é‡è¦ãªãƒ«ãƒ¼ãƒ«
- business_type ã¯ãã®ã¾ã¾ã€Œ{{business_type}}ã€ã¸å·®ã—è¾¼ã¿ï¼ˆè¨€ã„æ›ãˆä¸å¯ï¼‰ã€‚
- ã€Œï¼ˆã“ã“ã«{{strengths}}ã‚„{{values}}ã‚ˆã‚Šã€ã“ã®äº‹æ¥­æ‰€ã‚’ç§°ãˆã‚‹æ–‡ç« ã‚’ã„ã‚Œã¦ï¼‰ã€ã®éƒ¨åˆ†ã¯ã€
  strengths/values ã‹ã‚‰èª­ã¿å–ã‚Œã‚‹å…·ä½“ã‚’1ã€œ2ç‚¹ã ã‘ç¹”ã‚Šè¾¼ã‚“ã **è‡ªç„¶ãª1æ–‡**ã§ç½®æ›ã™ã‚‹ã“ã¨ã€‚
- å¼•ç”¨ç•ªå·ã‚„URLãƒªãƒ³ã‚¯ã¯æœ¬æ–‡ã«å…¥ã‚Œãªã„ã€‚
- å‡ºåŠ›ã¯æœ¬æ–‡ã®ã¿ï¼ˆã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ç¦æ­¢ï¼‰ã€‚

# å–¶æ¥­ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        {
            prompt_template
        }
    """.strip()

    # print("prompt:","="*80)
    # print(user_msg)
    # print("="*80)

    resp = client.chat.completions.create(
        model=model,
        messages=[
            {"role":"system", "content": system_msg},
            {"role":"user",   "content": user_msg},
        ],
        temperature=temperature,
        timeout=timeout,
    )

    text = resp.choices[0].message.content.strip()

    # ãƒ•ã‚§ãƒ³ã‚¹é™¤å» & ä½™è¨ˆãªã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å¯¾ç­–
    text = re.sub(r"^```(?:\w+)?\s*|\s*```$", "", text, flags=re.S).strip()
    return text


# %%
import sys
print("Python executable:", sys.executable)
print("Python version:", sys.version)
print("Python path:", sys.path[:3])  # æœ€åˆã®3ã¤ã®ãƒ‘ã‚¹ã‚’è¡¨ç¤º

# %% [markdown] id="KUT2pDUh-PoX"
# #### ãƒ†ã‚¹ãƒˆ

# %% colab={"base_uri": "https://localhost:8080/"} id="QuL4XRFB6Dbf" outputId="dbfda394-5be8-4f7e-a155-1e4232a0ef7d"
import json
urls = [
    # "https://www.2and4-auto.jp/",
    # "https://www.cardrshuu.com/#contact",
    # "http://www.nomiyama-car.com/contact/",
    # "https://h-bpc.com/",
    # "https://tauros-japan.com/",
    # "https://www.saitomotor-hirosaki.com/",
    # "https://kurumayakoubou.jp/",
]
for url in urls:
  resp = openai.responses.create(
    model="gpt-5-mini",  # ä¾‹ï¼šã‚³ã‚¹ãƒˆé‡è¦–ãªã‚‰ mini / å“è³ªé‡è¦–ãªã‚‰ gpt-5
    input=PROMPT_CLASSIFY.format(hp_url=url,vocab_list=", ".join(BUSINESS_TYPE_VOCAB)),
    tools=[{"type": "web_search"}]  # å†…è”µWebæ¤œç´¢ã‚’æœ‰åŠ¹åŒ–
  )
  comp_data = json.loads(resp.output_text)
  text = generate_sales_copy_with_infomation(comp_data, PROMPT, model="gpt-5-mini")
  print(text)
  # print("="*80)

# %% [markdown] id="bzGD3GSP6DJM"
# # å®Ÿè¡Œãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

# %% [markdown]
# ## dfã®æ ¼ç´

# %%
from pathlib import Path
import os, re
import pandas as pd

# Detect form-sales root on both VM and local

def get_form_sales_root() -> Path:
    env = os.getenv("FORM_SALES_ROOT")
    if env:
        p = Path(env)
        if p.exists():
            return p
    # Prefer module location to avoid CWD-induced duplicates like 'form-sales/form-sales'
    try:
        this_file = Path(__file__).resolve()
        # .../form-sales/src/form_url_fetch_msg_write_py/_01_helpers.py â†’ root = parents[2]
        module_root = this_file.parents[2]
        if (module_root / "src").exists():
            return module_root
    except Exception:
        pass
    # Fallback: Walk up to find a directory that looks like the project root
    cwd = Path.cwd()
    for p in [cwd, *cwd.parents]:
        # 1) If this dir itself is named 'form-sales' and has 'src', use it
        if p.name == "form-sales" and (p / "src").exists():
            return p
        # 2) If a child named 'form-sales' exists and has 'src', use it
        child = p / "form-sales"
        if child.exists() and child.is_dir() and (child / "src").exists():
            return child
    return cwd

def resolve_incoming_dir() -> Path:
    """Resolve incoming directory each call to avoid stale env values and ensure it exists."""
    env_dir = os.getenv("INCOMING_DIR")
    base = Path(env_dir) if env_dir else (get_form_sales_root() / "data" / "targets" / "incoming")
    # Normalize and ensure existence
    base = base.resolve()
    try:
        base.mkdir(parents=True, exist_ok=True)
    except Exception:
        # If creation fails (e.g., permission), continue; caller will error clearly
        pass
    return base

CLIENT_FILE_REGEX = re.compile(r"^(?P<client_id>[A-Za-z0-9_-]+)_\d{8}\.csv$", re.IGNORECASE)

def find_latest_incoming_csv(directory: Path) -> tuple[str, Path]:
    """
    Find the latest CSV matching pattern '<clientid>_YYYYMMDD.csv' in the given directory.
    Returns (client_id, file_path).
    """
    if not directory.exists():
        raise FileNotFoundError(f"Incoming directory not found: {directory}")
    candidates: list[tuple[str, Path]] = []
    for p in directory.glob("*.csv"):
        m = CLIENT_FILE_REGEX.match(p.name)
        if m:
            candidates.append((m.group("client_id"), p))
    if not candidates:
        raise FileNotFoundError(f"No CSV like '<clientid>_YYYYMMDD.csv' in: {directory}")
    # latest by modified time
    candidates.sort(key=lambda t: t[1].stat().st_mtime, reverse=True)
    return candidates[0]

def load_incoming_df() -> tuple[str, pd.DataFrame, Path]:
    """
    Resolve the incoming CSV path in a VM-friendly way and return (client_id, df, path).
    Priority:
      1) ENV INCOMING_DIR if provided
      2) auto-detected 'form-sales/data/targets/incoming'
    """
    incoming_dir = resolve_incoming_dir()
    client_id, p = find_latest_incoming_csv(incoming_dir)
    # read with UTF-8 BOM tolerant
    if not p.exists():
        raise FileNotFoundError(f"Input file not found: {p}")
    ext = p.suffix.lower()
    df = pd.read_excel(p) if ext in {".xlsx", ".xls"} else pd.read_csv(p, encoding="utf-8-sig")
    return client_id, df, p


# %% [markdown]
# ## å–¶æ¥­æ–‡ç« ç”Ÿæˆ

# %%
from datetime import datetime
import os
import json
import time
from typing import Optional, Iterable
import pandas as pd
from tqdm import tqdm
from openai import OpenAI
# äº‹å‰: !pip -q install tqdm openai

def _extract_json(text: str) -> str:
    s = text.strip()
    s = re.sub(r"^```(?:json)?\s*|\s*```$", "", s, flags=re.S).strip()
    m = re.search(r"\{[\s\S]*\}$", s)
    return (m.group(0) if m else s)


def fill_sales_copy_with_gpt(
    df: pd.DataFrame,
    *,
    url_col: str = "hp_url",
    out_col: str = "sales_copy",
    model: str = "gpt-5-mini",
    classify_prompt_template: str = None,   # ä¾‹: PROMPT_CLASSIFY
    sales_prompt_template: str = None,      # ä¾‹: PROMPT
    business_vocab: Optional[Iterable[str]] = None,  # ä¾‹: BUSINESS_TYPE_VOCAB
    overwrite: bool = True,
    sleep_sec: float = 0.8,
    openai_api_key: Optional[str] = None,
) -> pd.DataFrame:
    """
    å„è¡Œ: hp_url -> (åˆ†é¡JSON) -> generate_sales_copy_with_infomation -> sales_copy ã«æ ¼ç´
    é€²æ—ãƒãƒ¼ã¯1æœ¬ã ã‘è¡¨ç¤ºã€‚å–¶æ¥­æ–‡ç”Ÿæˆæ™‚ã« "record_created_at" ã‚’ "YYYY-MM-DD HH:MM:SS" ã§è¨˜éŒ²ã€‚
    """
    # å‡ºåŠ›åˆ—ã¨ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—åˆ—ã®ç”¨æ„
    if out_col not in df.columns:
        df.loc[:, out_col] = ""
    record_col = "record_created_at"
    if record_col not in df.columns:
        df.loc[:, record_col] = pd.NaT

    if (
        classify_prompt_template is None
        or sales_prompt_template is None
        or business_vocab is None
    ):
        raise ValueError(
            "classify_prompt_template / sales_prompt_template / business_vocab ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚"
        )

    client = OpenAI(api_key=openai_api_key or os.getenv("OPENAI_API_KEY", ""))
    mask = df[url_col].notna() & df[url_col].astype(str).str.strip().ne("")
    idxs = df[mask].index
    vocab_str = ", ".join(business_vocab)

    # --- é€²æ—ãƒãƒ¼ ---
    for i in tqdm(idxs, total=len(idxs), desc="å–¶æ¥­æ–‡ç”Ÿæˆ", unit="ç¤¾"):
        # æ—¢å­˜ã®å‡ºåŠ›ãŒã‚ã‚Šã€ã‹ã¤ä¸Šæ›¸ãã—ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if (not overwrite) and isinstance(df.at[i, out_col], str) and df.at[i, out_col].strip():
            continue

        url = str(df.at[i, url_col]).strip()
        try:
            # 1) åˆ†é¡ï¼ˆJSONç”Ÿæˆãƒ»Webæ¤œç´¢ONï¼‰
            prompt_cls = classify_prompt_template.format(hp_url=url, vocab_list=vocab_str)
            resp = client.responses.create(
                model=model,
                input=prompt_cls,
                tools=[{"type": "web_search"}],
            )
            comp_json = _extract_json(resp.output_text)
            comp_data = json.loads(comp_json)

            # 2) å–¶æ¥­æ–‡ç”Ÿæˆï¼ˆæ¤œç´¢ãªã—ï¼‰
            text = generate_sales_copy_with_infomation(
                company_info=comp_data,
                prompt_template=sales_prompt_template,
                model=model,
                temperature=1.0,
            )
            text_str = (text or "").strip()
            df.at[i, out_col] = text_str

            # ç”Ÿæˆã§ããŸå ´åˆã®ã¿ä½œæˆæ—¥æ™‚ã‚’è¨˜éŒ²ï¼ˆãƒ‡ãƒ¼ãƒˆå‹: ç§’ç²¾åº¦ï¼‰
            if text_str:
                df.at[i, record_col] = pd.Timestamp.now().floor("S")

        except Exception:
            # å¤±æ•—æ™‚ã¯å‡ºåŠ›ã‚’ç©ºã«ã—ã€ä½œæˆæ—¥æ™‚ã¯æ›´æ–°ã—ãªã„
            df.at[i, out_col] = ""
        time.sleep(sleep_sec)

    return df

# %% [markdown]
# ## Big queryæ›¸ãè¾¼ã¿

# %%
# %pip install -U google-cloud-bigquery google-cloud-bigquery-storage pandas-gbq db-dtypes pyarrow

# %%
# import google.auth
# creds, adc_proj = google.auth.default()
# from google.cloud import bigquery
# client = bigquery.Client()
# print("ADC:", adc_proj, "Client:", client.project)  # ä¸¡æ–¹ãŒ 469308 å´ã«ãªã£ã¦ã„ã‚Œã°OK

# %%
# # BigQuery: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/ãƒ†ãƒ¼ãƒ–ãƒ«è¨­å®šã¨ã‚¹ã‚­ãƒ¼ãƒç¢ºèª (â‘ ã‚«ãƒ©ãƒ ã‚’æ´—ã„å‡ºã™)
# import os
# import pandas as pd


# # 469308 å´ã‚’æ—¢å®šã«ã€‚å¿…è¦ãªã‚‰æ›¸ãæ›ãˆå¯
# GCLOUD_PROJECT_ID = os.getenv("GOOGLE_CLOUD_PROJECT") or os.getenv("GCLOUD_PROJECT") or "test-250817-469308"
# DATASET_ID = "dev"           # ã“ã“ã‚’å®Ÿç’°å¢ƒã«åˆã‚ã›ã¦å¤‰æ›´å¯
# TABLE_ID = "sales_list"      # ã“ã“ã‚’å®Ÿç’°å¢ƒã«åˆã‚ã›ã¦å¤‰æ›´å¯
# TABLE_FQN = f"{GCLOUD_PROJECT_ID}.{DATASET_ID}.{TABLE_ID}"

# client = bigquery.Client(project=GCLOUD_PROJECT_ID, location="asia-northeast1")

# # ã‚¹ã‚­ãƒ¼ãƒå–å¾—
# _table = client.get_table(TABLE_FQN)
# schema_df = pd.DataFrame([
#     {"name": f.name, "type": f.field_type, "mode": f.mode, "description": f.description}
#     for f in _table.schema
# ])
# print("TABLE:", TABLE_FQN)
# print(schema_df)



# %%
# from google.cloud import bigquery
# import pandas as pd

# # æ—¢å­˜ã® PROJECT_ID/DATASET_ID/TABLE_ID/TABLE_FQN ã¯ä¸Šã®ã‚»ãƒ«ã®å€¤ã‚’æµç”¨
# sent_at_value = "2020-01-01 00:00:00"  # DATETIMEç”¨ï¼ˆã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ãªã—ï¼‰

# data = [
#     {
#         "client_id": "TEST-001",
#         "recipient_company_name": "æ ªå¼ä¼šç¤¾ãƒ†ã‚¹ãƒˆ1",
#         "hp_url": "https://example.com/1",
#         "contact_url": "https://example.com/1/contact",
#         "sales_copy": "ã”æŒ¨æ‹¶1",
#         "record_created_at": sent_at_value,
#         "sent_at": sent_at_value,
#         "send_status": "draft",
#     },
#     {
#         "client_id": "TEST-002",
#         "recipient_company_name": "æ ªå¼ä¼šç¤¾ãƒ†ã‚¹ãƒˆ2",
#         "hp_url": "https://example.com/2",
#         "contact_url": "https://example.com/2/contact",
#         "sales_copy": "ã”æŒ¨æ‹¶2",
#         "record_created_at": sent_at_value,
#         "sent_at": sent_at_value,
#         "send_status": "draft",
#     },
#     {
#         "client_id": "TEST-003",
#         "recipient_company_name": "æ ªå¼ä¼šç¤¾ãƒ†ã‚¹ãƒˆ3",
#         "hp_url": "https://example.com/3",
#         "contact_url": "https://example.com/3/contact",
#         "sales_copy": "ã”æŒ¨æ‹¶3",
#         "record_created_at": sent_at_value,
#         "sent_at": sent_at_value,
#         "send_status": "draft",
#     },
# ]

# df = pd.DataFrame(data)
# insert_df = df[[c for c in df.columns if c in schema_df["name"].tolist()]].copy()

# # å¿µã®ãŸã‚ DATETIME ã¨ã—ã¦è§£é‡ˆã•ã›ã‚‹ï¼ˆtz ãªã—ï¼naiveï¼‰
# insert_df["sent_at"] = pd.to_datetime(insert_df["sent_at"]).dt.tz_localize(None)
# insert_df["record_created_at"] = pd.to_datetime(insert_df["record_created_at"]).dt.tz_localize(None)

# client = bigquery.Client(project=GCLOUD_PROJECT_ID, location="asia-northeast1")
# job = client.load_table_from_dataframe(
#     insert_df,
#     TABLE_FQN,
#     bigquery.LoadJobConfig(write_disposition="WRITE_APPEND")
# )
# job.result()
# print("loaded rows:", len(insert_df))

# # æ¤œè¨¼ï¼ˆä»Šå…¥ã‚ŒãŸ client_id ã ã‘å–å¾—ï¼‰
# keys = insert_df["client_id"].tolist()
# qry = f"""
# SELECT client_id, recipient_company_name, hp_url, contact_url, sales_copy, sent_at, record_created_at, send_status
# FROM `{TABLE_FQN}`
# WHERE client_id IN UNNEST(@keys)
# ORDER BY client_id
# """
# res = client.query(
#     qry,
#     job_config=bigquery.QueryJobConfig(
#         query_parameters=[bigquery.ArrayQueryParameter("keys", "STRING", keys)]
#     ),
# ).result().to_dataframe()
# print(res)

# %%
def prepare_contact_url_filled_df_for_bq(
    df: pd.DataFrame,
    *,
    client_id: Optional[str] = None,
    send_status_value: str = "æœªé€ä¿¡",
    sent_at_value: str = "2020-01-01 00:00:00",
) -> pd.DataFrame:
    """
    contact_url_filled_dfï¼ˆå–¶æ¥­æ–‡ç”Ÿæˆå¾Œã®DFï¼‰ã‚’ BigQueryæŠ•å…¥ç”¨ã«æ•´å½¢ã™ã‚‹ã€‚

    å®Ÿæ–½å†…å®¹:
      - åˆ—åå¤‰æ›´: "company_name" â†’ "recipient_company_name"
      - åˆ—è¿½åŠ :  "client_id"ï¼ˆæŒ‡å®šãŒã‚ã‚Œã°å…¨è¡Œã«è¨­å®šã€æœªæŒ‡å®šæ™‚ã¯ç©ºæ–‡å­—ï¼‰
      - åˆ—è¿½åŠ :  "send_status"ï¼ˆå…¨è¡Œ "æœªé€ä¿¡"ï¼‰
      - åˆ—è¿½åŠ :  "sent_at"ï¼ˆå…¨è¡Œ "2020-01-01 00:00:00"ï¼‰

    æ—¢å­˜åˆ—ãŒã‚ã‚‹å ´åˆã‚‚ã€"send_status" ã¨ "sent_at" ã¯æŒ‡å®šã®å®šæ•°ã§ä¸Šæ›¸ãã™ã‚‹ã€‚
    å…ƒã® DataFrame ã¯å¤‰æ›´ã›ãšã€åŠ å·¥å¾Œã®ã‚³ãƒ”ãƒ¼ã‚’è¿”ã™ã€‚
    """
    out = df.copy()

    # 1) åˆ—åå¤‰æ›´ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ï¼‰
    if "company_name" in out.columns and "recipient_company_name" not in out.columns:
        out = out.rename(columns={"company_name": "recipient_company_name"})

    # 2) client_id åˆ—ã®ç”¨æ„ã¨è¨­å®š
    if "client_id" not in out.columns:
        out.loc[:, "client_id"] = ""
    if client_id is not None:
        out.loc[:, "client_id"] = str(client_id)

    # 3) send_status / sent_at ã‚’å®šæ•°ã§ä»˜ä¸ï¼ˆä¸Šæ›¸ãï¼‰
    out.loc[:, "send_status"] = send_status_value
    out.loc[:, "sent_at"] = sent_at_value

    return out


# %% [markdown]
# ## BQã¸æ ¼ç´

# %%
# from __future__ import annotations

from typing import Iterable, Mapping, Optional
import pandas as pd
from google.cloud import bigquery


def load_sales_list_df_to_bq(
    df: pd.DataFrame,
    *,
    project_id: str,
    dataset_id: str = "dev",
    table_id: str = "sales_list",
    location: str = "asia-northeast1",
    write_disposition: str = "WRITE_APPEND",
    require_all_columns: bool = True,
) -> int:
    """
    Load a DataFrame to BigQuery table `{project_id}.{dataset_id}.{table_id}` with
    schema alignment for the following columns:
      - client_id (STRING, REQUIRED)
      - recipient_company_name (STRING, REQUIRED)
      - hp_url (STRING, REQUIRED)
      - contact_url (STRING, REQUIRED)
      - sales_copy (STRING, REQUIRED)
      - record_created_at (DATETIME, REQUIRED)
      - sent_at (DATETIME, REQUIRED)
      - send_status (STRING, REQUIRED)

    Notes on authentication for VM:
    - If this runs on a GCE/Cloud Run/Composer VM with an attached service account,
      the BigQuery client will automatically use Application Default Credentials (ADC).
      No human login is required if the service account has `roles/bigquery.dataEditor`
      (or appropriate) on the target dataset/table.
    - Locally, you can also set GOOGLE_APPLICATION_CREDENTIALS to a service account JSON.

    Returns: number of rows loaded.
    """
    required_columns = [
        "client_id",
        "recipient_company_name",
        "hp_url",
        "contact_url",
        "sales_copy",
        "record_created_at",
        "sent_at",
        "send_status",
    ]

    # 1) å¿…é ˆåˆ—ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
    missing = [c for c in required_columns if c not in df.columns]
    if missing and require_all_columns:
        raise ValueError(f"Missing required columns: {missing}")

    # 2) ã‚¹ã‚­ãƒ¼ãƒé †ã«ãã‚ãˆãŸã‚³ãƒ”ãƒ¼ã‚’ä½œæˆï¼ˆä½™åˆ†åˆ—ã¯è½ã¨ã™ï¼‰
    use_cols = [c for c in required_columns if c in df.columns]
    insert_df = df[use_cols].copy()

    # 3) å‹ã®æ­£è¦åŒ–
    string_cols = [
        "client_id",
        "recipient_company_name",
        "hp_url",
        "contact_url",
        "sales_copy",
        "send_status",
    ]
    for col in string_cols:
        if col in insert_df:
            insert_df[col] = insert_df[col].astype("string").fillna("")

    # DATETIMEï¼ˆtz ãªã—ï¼‰ã¸å¤‰æ›ã€‚None/ç©ºæ–‡å­—/"None" ã¯ NaT ã«ãªã‚‹ã®ã§äº‹å‰ã«å¼¾ããªã‚‰ã“ã“ã§å¯¾å¿œ
    for col in ["record_created_at", "sent_at"]:
        if col in insert_df:
            series = (
                insert_df[col]
                .replace({"None": None})
                .astype("string")
                .where(lambda s: s.str.strip().ne(""), None)
            )
            insert_df[col] = pd.to_datetime(series, errors="coerce")
            # BigQuery DATETIME ã¯ tz ãªã—ã€NaT ã¯è¨±å®¹ã—ãªã„ã®ã§æœ€çµ‚ãƒã‚§ãƒƒã‚¯
            if insert_df[col].isna().any():
                bad_idx = insert_df[col][insert_df[col].isna()].index.tolist()
                raise ValueError(f"Invalid DATETIME in column '{col}' at rows: {bad_idx[:10]} ...")

    table_fqn = f"{project_id}.{dataset_id}.{table_id}"

    client = bigquery.Client(project=project_id, location=location)
    job = client.load_table_from_dataframe(
        insert_df,
        table_fqn,
        job_config=bigquery.LoadJobConfig(write_disposition=write_disposition),
    )
    job.result()
    return len(insert_df)


